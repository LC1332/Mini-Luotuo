{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/Mini-Luotuo/blob/main/3.5B_minimal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3lytinmcl_Y"
      },
      "source": [
        "# 迷你骆驼:一系列蒸馏指令数据得到的中文语言模型\n",
        "\n",
        "[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)]()\n",
        "[![Data License](https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg)]()\n",
        "\n",
        "<p align=\"center\"> <a href=\"https://scholar.google.com/citations?user=imroB-8AAAAJ&hl=zh-CN\" target=\"_blank\">黄钟健 @ 西安电子科大</a>, <a href=\"https://github.com/LC1332\" target=\"_blank\">李鲁鲁 @ 商汤科技</a></p>\n",
        "\n",
        "\n",
        "<details>\n",
        "  <summary> 黄钟健 训练了本项目的第一个模型 </summary>\n",
        "\n",
        "李鲁鲁发起了项目，并提出了后续使用feature进行蒸馏的思路。\n",
        "\n",
        "黄钟健 训练了本项目的第一个模型，并且实现了训练框架\n",
        "\n",
        "后续如果更多其他的同学训练小模型，我们也会陆续加入到作者列表中\n",
        "\n",
        "</details>\n",
        "\n",
        "项目地址https://github.com/LC1332/Mini-Luotuo\n",
        "\n",
        "本notebook由黄钟健实现\n",
        "\n",
        "目前包含3.5B模型的inference代码"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sRpuBYV3cuda"
      },
      "source": [
        "安装必要的环境"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9deFaJWjaJ2-"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes datasets sentencepiece transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jTgZI1c-avja"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "def generate_prompt(instruction, input=None):\n",
        "    if input:\n",
        "        return f\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
        "    else:\n",
        "        return f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "auuhPiF3a0GJ"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('Midkey/GPT2-312M-chinese-ft-BertTokenizer-luotuo')\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained('Midkey/GPT2-312M-chinese-ft-BertTokenizer-luotuo', trust_remote_code=True, load_in_8bit=True, device_map='auto')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "L24kOqovbIsk"
      },
      "outputs": [],
      "source": [
        "def evaluate(\n",
        "    instruction,\n",
        "    input=None,\n",
        "    max_new_tokens=256,\n",
        "):\n",
        "    prompt = generate_prompt(instruction, input)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
        "    input_ids = inputs[\"input_ids\"].cuda()\n",
        "    eos_token_id = 104\n",
        "    with torch.no_grad():\n",
        "        generation_output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            eos_token_id=eos_token_id\n",
        "        )\n",
        "    for s in generation_output.sequences:\n",
        "        decode_s = s[len(input_ids[0]):]\n",
        "        if decode_s[-1] == eos_token_id:\n",
        "            decode_s = decode_s[:-1]\n",
        "        output = tokenizer.decode(decode_s)\n",
        "        print(\"Response:\", output)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dKFJtc96eeBa"
      },
      "source": [
        "现在可以玩了"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RxAH_WDbK8o"
      },
      "outputs": [],
      "source": [
        "while 1:\n",
        "    instruction = input('Ask something: ')\n",
        "    evaluate(instruction)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5pEvO0MHegPD"
      },
      "source": [
        "TODO\n",
        "\n",
        "- [ ] 一个gradio的游玩GUI\n",
        "- [ ] 更好的蒸馏策略"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Urhvunyuel9T"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
